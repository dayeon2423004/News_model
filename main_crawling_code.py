# ï¼‘ï¼•ï¼ä»¶åé›†ã‚³ãƒ¼ãƒ‰

import csv
import os
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
import time

# =====================
# è¨­å®š
# =====================
output_file = "v2_collected_articles.csv"
max_total = 150  # åˆè¨ˆç›®æ¨™ä»¶æ•°
page_num = 1
collected_data = []
existing_titles = set()
fail_count = 0
max_fails = 10  # é€£ç¶š10å›å¤±æ•—ã—ãŸã‚‰æ­¢ã‚ã‚‹

# =====================
# æ—¢å­˜ã®CSVèª­ã¿è¾¼ã¿
# =====================
if os.path.exists(output_file):
    with open(output_file, "r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            existing_titles.add(row["title"])
            collected_data.append(row)

print(f"ğŸ” æ—¢å­˜è¨˜äº‹æ•°: {len(collected_data)}ä»¶")

# =====================
# ãƒ–ãƒ©ã‚¦ã‚¶è¨­å®š
# =====================
options = webdriver.ChromeOptions()
options.add_argument("--headless")
driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
driver.implicitly_wait(10)

try:
    while len(collected_data) < max_total:
        url = f"https://www.yna.co.kr/news?site=navi_news&p={page_num}"
        print(f"\nğŸ“„ ãƒšãƒ¼ã‚¸ç§»å‹•: {url}")
        driver.get(url)
        time.sleep(2)

        articles = driver.find_elements(By.CSS_SELECTOR, "a[href*='/view/']")
        article_data = []

        for article in articles:
            link = article.get_attribute("href")
            title = article.text.strip()
            if title and title not in existing_titles:
                article_data.append((title, link))

        success_in_page = 0  # â†ã“ã®ãƒšãƒ¼ã‚¸ã§æˆåŠŸã—ãŸä»¶æ•°

        for title, link in article_data:
            try:
                driver.get(link)
                time.sleep(2)
            except Exception as e:
                print(f"âŒ ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—ï¼ˆ{title}ï¼‰: {e}")
                continue

            content = None
            try:
                content_element = driver.find_element(By.CSS_SELECTOR, ".article-text")
                content = content_element.text
            except:
                try:
                    content_element = driver.find_element(By.CSS_SELECTOR, ".story-news")
                    content = content_element.text
                except:
                    print(f"âŒ æœ¬æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆ{title}ï¼‰")
                    continue

            try:
                date_element = driver.find_element(By.CSS_SELECTOR, ".update-time")
                date_text = date_element.text.strip()
            except:
                date_text = "æ—¥ä»˜ãªã—"

            if content:
                collected_data.append({
                    "title": title,
                    "date": date_text,
                    "content": content,
                    "url": link
                })
                existing_titles.add(title)
                success_in_page += 1
                print(f"âœ… {len(collected_data)}ä»¶ç›®: {title}")

            if len(collected_data) >= max_total:
                break

        if success_in_page == 0:
            fail_count += 1
            print(f"âš ï¸ ã“ã®ãƒšãƒ¼ã‚¸ã§æˆåŠŸã‚¼ãƒ­ã€‚é€£ç¶šå¤±æ•—æ•°: {fail_count}")
        else:
            fail_count = 0

        if fail_count >= max_fails:
            print("âš ï¸ é€£ç¶šå¤±æ•—ãŒå¤šã„ãŸã‚ã€å®‰å…¨ã®ãŸã‚åœæ­¢ã—ã¾ã™ã€‚")
            raise KeyboardInterrupt

        page_num += 1

except KeyboardInterrupt:
    print("ğŸ›‘ ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒä¸­æ–­ã—ã¾ã—ãŸ")

except Exception as e:
    print(f"ğŸ”¥ äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}")

finally:
    driver.quit()
    with open(output_file, "w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=["title", "date", "content", "url"])
        writer.writeheader()
        writer.writerows(collected_data)

    print(f"ğŸ‰ å®Œäº†ï¼åˆè¨ˆ {len(collected_data)} ä»¶ã‚’CSVã«ä¿å­˜ã—ã¾ã—ãŸï¼ˆé‡è¤‡ãªã—ï¼‰ï¼")
