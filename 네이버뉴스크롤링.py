from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
import time
import pandas as pd
from bs4 import BeautifulSoup
import random
import re
from datetime import datetime
import requests # FestAPI ì„œë²„ë¡œ ìš”ì²­ë³´ë‚´ê¸° ìœ„í•œ import

def crawl_articles():
    # í¬ë¡¬ ë“œë¼ì´ë²„ ì„¤ì •
    options = Options()
    options.add_argument('--start-maximized')
    driver = webdriver.Chrome(options=options)

    # ì¡°ì„ ì¼ë³´ ì ‘ì†
    driver.get("https://www.chosun.com/")
    time.sleep(2)

    # ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘
    elements = driver.find_elements(By.CSS_SELECTOR, "a.story-card__headline")
    links = []

    for el in elements:
        href = el.get_attribute("href")
        if href and href.startswith("https://www.chosun.com") and href not in links:
            links.append(href)

    print(f"ğŸ“° ìˆ˜ì§‘ëœ ê¸°ì‚¬ ë§í¬ ìˆ˜: {len(links)}")

    # ë‚ ì§œê°€ ìˆëŠ” ê¸°ì‚¬ë§Œ ìµœëŒ€ 5ê°œ ìˆ˜ì§‘
    valid_articles = []
    random.shuffle(links)

    for link in links:
        if len(valid_articles) >= 5:
            break

        driver.get(link)
        time.sleep(2)
        soup = BeautifulSoup(driver.page_source, "html.parser")

        try:
            # ì œëª©
            title_tag = soup.select_one(".article-header__headline span")
            title = title_tag.text.strip() if title_tag else "ì œëª© ì—†ìŒ"

            # ë³¸ë¬¸
            body_paragraphs = soup.find_all("p", class_=lambda c: c and "article-body__content-text" in c)
            body = "\n".join(p.text.strip() for p in body_paragraphs if p.text.strip())

            # ë‚ ì§œ ì¶”ì¶œ
            date_block = soup.select_one(".upDate")
            publish_date = None
            if date_block:
                raw_text = date_block.get_text(separator=" ", strip=True)
                date_match = re.search(r"\d{4}\.\d{2}\.\d{2}", raw_text)
                if date_match:
                    # ë‚ ì§œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    publish_date = datetime.strptime(date_match.group(0), "%Y.%m.%d")

            if not publish_date:
                print(f"âš ï¸ ë‚ ì§œ ì—†ìŒ â†’ ê¸°ì‚¬ ìŠ¤í‚µ: {link}")
                continue

            print(f"\nğŸ“Œ ì œëª©: {title}")
            print(f"ğŸ—“ï¸ ë‚ ì§œ: {publish_date.date()}")
            print(f"ğŸ”— URL: {link}")
            print(f"ğŸ“° ë³¸ë¬¸ ì¼ë¶€:\n{body[:300]}...\n{'-'*60}")

            valid_articles.append({
                "ì œëª©": title,
                "ë³¸ë¬¸": body,
                "URL": link,
                "ë‚ ì§œ": publish_date
            })

        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {link} - {e}")
            continue

    driver.quit()

    for article in valid_articles:
        response = requests.post("http://localhost:8000/summarize/audio", json={"url": article["URL"]})
        
        if response.status_code == 200:
            print(f"âœ… ì²˜ë¦¬ ì„±ê³µ: {article['ì œëª©']}")
        else:
            print(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {article['ì œëª©']} - {response.text}")


# ì¶”ê°€ ì •ë³´ : FestAPIì„œë²„ì—ì„œëŠ” uvicornìœ¼ë¡œ ì‹¤í–‰í•´ì•¼ í•¨. (uvicorn api_voice:app --reload)


# # ì €ì¥
# df = pd.DataFrame(valid_articles)
# df.to_csv("chosun_articles_with_date.csv", index=False, encoding='utf-8-sig', date_format="%Y-%m-%d")
# print("âœ… ë‚ ì§œ í¬í•¨ëœ 5ê°œ ê¸°ì‚¬ ì €ì¥ ì™„ë£Œ: chosun_articles_with_date.csv")
