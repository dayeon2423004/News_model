from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast

# 토크나이저와 모델 로드
tokenizer = PreTrainedTokenizerFast.from_pretrained("gogamza/kobart-base-v2")
model = BartForConditionalGeneration.from_pretrained("gogamza/kobart-base-v2")

# 입력
input_string = """대법원 전원 합의체가 전날 이재명 더불어민주당 대선 후보의 공직선거법 위반 사건 상고심에서 무죄를 선고했던 2심을 파기하고 유죄 취지로 사건을 서울고법으로 돌려보낸 것과 관련해 민주당 의원들은 2일 “사법 쿠데타” “제2의 인혁당 사건”이라고 비판했다. 이에 대해 대법원 측은 “최고 법원의 결정은 제도적으로 존중돼야 한다”고 반박했고, 국민의힘도 “지극히 상식적인 판단”이라고 했다.
이날 국회 법제사법위원회는 전날 대법원 선고와 관련해 긴급현안질의를 열었다. 고검장 출신으로 과거 이 후보 변호인을 맡았던 박균택 민주당 의원은 “이승만 전 대통령이 정적 조봉암 선생에 대해 사형 판결을 해 사법 살인을 저지른 후 벌어진 최대의 대선 개입 사건”이라면서 “대법원에 의한 사법 쿠데타”라고 주장했다.
같은 당 이성윤 의원도 “법원이 작심하고 선거에 개입한 것”이라며 “제2의 인혁당 사건보다 더 심각한 것”이라고 했다. “인혁당 사건은 유신 독재권력에 굴복한 것이지만 이것은 법원이 자원에서 한 것”이라며 “법원이 어떤 꼼수를 부렸는지 국민들은 다 안다”고 주장했다.
같은 당 박희승 의원도 “사건 기록이 6만~7만쪽이 넘는 걸로 아는데 대법관님들이 기록을 다 읽어 보셨냐”면서 “이 후보는 유력한 대선 주자이고 국민의 선택을 앞두고 있는데, 이런 상황에서 서둘러 재판을 진행한 것이 타당한지 의문”이라고 했다.
이에 대해 천대엽 법원행정처장은 “판결에 대한 비판·비평은 얼마든지 가능하지만 최고 법원의 판결과 법관에 대한 존중 없이는 법치주의도, 이를 뒷받침하는 헌법기관도 존재할 수 없다고 생각한다”고 했다.
배형원 법원행정처 차장이 2일 오전 서울 여의도 국회에서 열린 법제사법위원회 전체회의에서 이재명 더불어민주당 대통령 후보 공직선거법 위반 사건과 관련한 대법원의 유죄취지 파기환송 판결 관련 질의에 답변하고 있는 가운데 조희대 대법원장을 비롯한 파기환송에 찬성한 대법관들의 얼굴 사진이 나오고 있다./뉴스1
이어 “(전원 합의체는) 대법원장이 좌지우지할 수 있는 구조가 전혀 아니다”면서 “(이 후보의 김문기·성남시 발언이) 허위인지에 대한 실체적·법리적 쟁점에 대해 충실히 논의가 이뤄지고 판결에 다 담아서 90페이지에 가까운 판결문이 나온 것으로 안다”고 했다.
배형원 법원행정처 차장도 “판결에 대한 평가는 다양할 수 있지만 최고법원의 결정에 대해서는 제도적으로 존중돼야 한다고 생각한다”고 했다. ‘피고인이 정치인이면 대법관들이 지위나 이념 성향에 따라 사건을 판단하냐’는 조배숙 국민의힘 의원의 질의에는 “그렇지 않다”고 답했다.
조 의원은 “민주당이 사법부 판단의 정당성을 부정하고, (이 후보를) 정치적 희생양으로 포장하려는 시도에 깊은 우려를 표명한다”며 “이 후보는 지금이라도 국민 앞에 책임 있는 자세를 취하고 후보직에서 자진 사퇴해야 한다”고 했다.
같은 당 주진우 의원은 “국회가 (전날 대법원 선고 후) 곧바로 현안질의를 통해 판결의 구체적인 내용을 이야기하고 법원을 압박하는 건 입법부가 사법부를 침탈하는 모습”이라고 비판했다.
장동혁 국민의힘 의원은 “법원이 이 후보 재판 지연에 제대로 대응하지 못하고 엄정한 조치를 하지 않았기 때문에 1심이 2년 이상 늘어졌던 것”이라며 “지극히 상식적인 판단을 두고 긴 시간이 걸려 최종적인 결론을 내리지 못한 것에 대해 안타깝게 생각한다”고 말했다."""

# 토크나이저로 변환( 숫자 토큰으로 변환하여 모델이 이해할 수 있게)
token = tokenizer(
    input_string, # 요약할 문장
    return_tensors="pt", # pytorch용 텐서 변환
    max_length=512, # 입력 텍스트의 토큰 개수 제한(512가 최대)
    truncation=True # 길면 자르기
    )


# 자동으로 tokenizer가 생성한 token_type_ids 제거 (KoBART에서는 사용하지 않음)
if "token_type_ids" in token:
    del token["token_type_ids"]


# 모델이 확인해 요약 토큰 생성(딕셔너리이기에 앞에 **를 붙여 언팩킹. 리스트 언팩킹은 *만 붙임.)
# 딕셔너리를 인자로 주는 것이 아닌 언팩킹을 하여 하나씩 인자를 줌.
model_summary = model.generate(
    **token, 
    no_repeat_ngram_size=3, # 3-gram 반복 방지
     repetition_penalty=2.0,  # 반복 어구에 불이익
    max_new_tokens=128   # 모델 요약 출력 토큰 명시적 설정하지 않을시 최대가 20임. 따라서 최대 128로 명시해줌.
    ) # 결과는 2차원 메트릭스. (문장 개수, 토큰 개수)

# 사람이 읽을 수 있게 토근 디코딩
# skip_special_tokens=True 은 특수 토큰은 제거.
summary = tokenizer.decode(model_summary[0], skip_special_tokens=True)

# 요약된 문장 확인
print(summary)
